{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "flaml-hello-world.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPYPb96R8mNcBJcyhG8U9xf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/noahgift/flaml-nba/blob/main/flaml_hello_world.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcvAWJnDz64S"
      },
      "source": [
        "!pip install -q flaml"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evH7nB1l1ala",
        "outputId": "44e1bbe5-e44c-49e3-9a65-3e90bef030f0"
      },
      "source": [
        "from flaml import AutoML\n",
        "from sklearn.datasets import load_iris\n",
        "# Initialize an AutoML instance\n",
        "automl = AutoML()\n",
        "# Specify automl goal and constraint\n",
        "automl_settings = {\n",
        "    \"time_budget\": 10,  # in seconds\n",
        "    \"metric\": 'accuracy',\n",
        "    \"task\": 'classification',\n",
        "}\n",
        "X_train, y_train = load_iris(return_X_y=True)\n",
        "# Train with labeled input data\n",
        "automl.fit(X_train=X_train, y_train=y_train,\n",
        "           **automl_settings)\n",
        "# Predict\n",
        "print(automl.predict_proba(X_train))\n",
        "# Export the best model\n",
        "print(automl.model)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[flaml.automl: 06-16 17:20:03] {908} INFO - Evaluation method: cv\n",
            "[flaml.automl: 06-16 17:20:03] {607} INFO - Using StratifiedKFold\n",
            "[flaml.automl: 06-16 17:20:03] {929} INFO - Minimizing error metric: 1-accuracy\n",
            "[flaml.automl: 06-16 17:20:03] {949} INFO - List of ML learners in AutoML Run: ['lgbm', 'rf', 'catboost', 'xgboost', 'extra_tree', 'lrl1']\n",
            "[flaml.automl: 06-16 17:20:03] {1013} INFO - iteration 0, current learner lgbm\n",
            "[flaml.automl: 06-16 17:20:03] {1165} INFO -  at 0.1s,\tbest lgbm's error=0.0733,\tbest lgbm's error=0.0733\n",
            "[flaml.automl: 06-16 17:20:03] {1013} INFO - iteration 1, current learner lgbm\n",
            "[flaml.automl: 06-16 17:20:03] {1165} INFO -  at 0.1s,\tbest lgbm's error=0.0733,\tbest lgbm's error=0.0733\n",
            "[flaml.automl: 06-16 17:20:03] {1013} INFO - iteration 2, current learner lgbm\n",
            "[flaml.automl: 06-16 17:20:03] {1165} INFO -  at 0.1s,\tbest lgbm's error=0.0667,\tbest lgbm's error=0.0667\n",
            "[flaml.automl: 06-16 17:20:03] {1013} INFO - iteration 3, current learner lgbm\n",
            "[flaml.automl: 06-16 17:20:03] {1165} INFO -  at 0.2s,\tbest lgbm's error=0.0467,\tbest lgbm's error=0.0467\n",
            "[flaml.automl: 06-16 17:20:03] {1013} INFO - iteration 4, current learner lgbm\n",
            "[flaml.automl: 06-16 17:20:03] {1165} INFO -  at 0.2s,\tbest lgbm's error=0.0467,\tbest lgbm's error=0.0467\n",
            "[flaml.automl: 06-16 17:20:03] {1013} INFO - iteration 5, current learner lgbm\n",
            "[flaml.automl: 06-16 17:20:03] {1165} INFO -  at 0.3s,\tbest lgbm's error=0.0400,\tbest lgbm's error=0.0400\n",
            "[flaml.automl: 06-16 17:20:03] {1013} INFO - iteration 6, current learner lgbm\n",
            "[flaml.automl: 06-16 17:20:03] {1165} INFO -  at 0.3s,\tbest lgbm's error=0.0400,\tbest lgbm's error=0.0400\n",
            "[flaml.automl: 06-16 17:20:03] {1013} INFO - iteration 7, current learner lgbm\n",
            "[flaml.automl: 06-16 17:20:03] {1165} INFO -  at 0.3s,\tbest lgbm's error=0.0400,\tbest lgbm's error=0.0400\n",
            "[flaml.automl: 06-16 17:20:03] {1013} INFO - iteration 8, current learner lgbm\n",
            "[flaml.automl: 06-16 17:20:03] {1165} INFO -  at 0.4s,\tbest lgbm's error=0.0400,\tbest lgbm's error=0.0400\n",
            "[flaml.automl: 06-16 17:20:03] {1013} INFO - iteration 9, current learner xgboost\n",
            "[flaml.automl: 06-16 17:20:03] {1165} INFO -  at 0.5s,\tbest xgboost's error=0.0600,\tbest lgbm's error=0.0400\n",
            "[flaml.automl: 06-16 17:20:03] {1013} INFO - iteration 10, current learner lgbm\n",
            "[flaml.automl: 06-16 17:20:03] {1165} INFO -  at 0.6s,\tbest lgbm's error=0.0400,\tbest lgbm's error=0.0400\n",
            "[flaml.automl: 06-16 17:20:03] {1013} INFO - iteration 11, current learner extra_tree\n",
            "[flaml.automl: 06-16 17:20:05] {1165} INFO -  at 1.7s,\tbest extra_tree's error=0.0667,\tbest lgbm's error=0.0400\n",
            "[flaml.automl: 06-16 17:20:05] {1013} INFO - iteration 12, current learner rf\n",
            "[flaml.automl: 06-16 17:20:06] {1165} INFO -  at 2.9s,\tbest rf's error=0.0600,\tbest lgbm's error=0.0400\n",
            "[flaml.automl: 06-16 17:20:06] {1013} INFO - iteration 13, current learner xgboost\n",
            "[flaml.automl: 06-16 17:20:06] {1165} INFO -  at 2.9s,\tbest xgboost's error=0.0600,\tbest lgbm's error=0.0400\n",
            "[flaml.automl: 06-16 17:20:06] {1013} INFO - iteration 14, current learner lgbm\n",
            "[flaml.automl: 06-16 17:20:06] {1165} INFO -  at 2.9s,\tbest lgbm's error=0.0400,\tbest lgbm's error=0.0400\n",
            "[flaml.automl: 06-16 17:20:06] {1013} INFO - iteration 15, current learner xgboost\n",
            "[flaml.automl: 06-16 17:20:06] {1165} INFO -  at 3.0s,\tbest xgboost's error=0.0600,\tbest lgbm's error=0.0400\n",
            "[flaml.automl: 06-16 17:20:06] {1013} INFO - iteration 16, current learner lgbm\n",
            "[flaml.automl: 06-16 17:20:06] {1165} INFO -  at 3.0s,\tbest lgbm's error=0.0400,\tbest lgbm's error=0.0400\n",
            "[flaml.automl: 06-16 17:20:06] {1013} INFO - iteration 17, current learner xgboost\n",
            "[flaml.automl: 06-16 17:20:06] {1165} INFO -  at 3.0s,\tbest xgboost's error=0.0600,\tbest lgbm's error=0.0400\n",
            "[flaml.automl: 06-16 17:20:06] {1013} INFO - iteration 18, current learner lgbm\n",
            "[flaml.automl: 06-16 17:20:06] {1165} INFO -  at 3.1s,\tbest lgbm's error=0.0400,\tbest lgbm's error=0.0400\n",
            "[flaml.automl: 06-16 17:20:06] {1013} INFO - iteration 19, current learner xgboost\n",
            "[flaml.automl: 06-16 17:20:06] {1165} INFO -  at 3.1s,\tbest xgboost's error=0.0600,\tbest lgbm's error=0.0400\n",
            "[flaml.automl: 06-16 17:20:06] {1013} INFO - iteration 20, current learner rf\n",
            "[flaml.automl: 06-16 17:20:07] {1165} INFO -  at 4.3s,\tbest rf's error=0.0600,\tbest lgbm's error=0.0400\n",
            "[flaml.automl: 06-16 17:20:07] {1013} INFO - iteration 21, current learner xgboost\n",
            "[flaml.automl: 06-16 17:20:07] {1165} INFO -  at 4.3s,\tbest xgboost's error=0.0600,\tbest lgbm's error=0.0400\n",
            "[flaml.automl: 06-16 17:20:07] {1013} INFO - iteration 22, current learner lgbm\n",
            "[flaml.automl: 06-16 17:20:07] {1165} INFO -  at 4.3s,\tbest lgbm's error=0.0400,\tbest lgbm's error=0.0400\n",
            "[flaml.automl: 06-16 17:20:07] {1013} INFO - iteration 23, current learner lgbm\n",
            "[flaml.automl: 06-16 17:20:07] {1165} INFO -  at 4.4s,\tbest lgbm's error=0.0400,\tbest lgbm's error=0.0400\n",
            "[flaml.automl: 06-16 17:20:07] {1013} INFO - iteration 24, current learner xgboost\n",
            "[flaml.automl: 06-16 17:20:07] {1165} INFO -  at 4.4s,\tbest xgboost's error=0.0600,\tbest lgbm's error=0.0400\n",
            "[flaml.automl: 06-16 17:20:07] {1013} INFO - iteration 25, current learner extra_tree\n",
            "[flaml.automl: 06-16 17:20:08] {1165} INFO -  at 5.6s,\tbest extra_tree's error=0.0400,\tbest lgbm's error=0.0400\n",
            "[flaml.automl: 06-16 17:20:08] {1013} INFO - iteration 26, current learner lgbm\n",
            "[flaml.automl: 06-16 17:20:08] {1165} INFO -  at 5.6s,\tbest lgbm's error=0.0400,\tbest lgbm's error=0.0400\n",
            "[flaml.automl: 06-16 17:20:08] {1013} INFO - iteration 27, current learner xgboost\n",
            "[flaml.automl: 06-16 17:20:08] {1165} INFO -  at 5.7s,\tbest xgboost's error=0.0467,\tbest lgbm's error=0.0400\n",
            "[flaml.automl: 06-16 17:20:08] {1013} INFO - iteration 28, current learner xgboost\n",
            "[flaml.automl: 06-16 17:20:09] {1165} INFO -  at 5.7s,\tbest xgboost's error=0.0467,\tbest lgbm's error=0.0400\n",
            "[flaml.automl: 06-16 17:20:09] {1013} INFO - iteration 29, current learner extra_tree\n",
            "[flaml.automl: 06-16 17:20:10] {1165} INFO -  at 7.0s,\tbest extra_tree's error=0.0400,\tbest lgbm's error=0.0400\n",
            "[flaml.automl: 06-16 17:20:10] {1013} INFO - iteration 30, current learner lgbm\n",
            "[flaml.automl: 06-16 17:20:10] {1165} INFO -  at 7.0s,\tbest lgbm's error=0.0400,\tbest lgbm's error=0.0400\n",
            "[flaml.automl: 06-16 17:20:10] {1013} INFO - iteration 31, current learner lgbm\n",
            "[flaml.automl: 06-16 17:20:10] {1165} INFO -  at 7.0s,\tbest lgbm's error=0.0400,\tbest lgbm's error=0.0400\n",
            "[flaml.automl: 06-16 17:20:10] {1013} INFO - iteration 32, current learner lgbm\n",
            "[flaml.automl: 06-16 17:20:10] {1165} INFO -  at 7.1s,\tbest lgbm's error=0.0400,\tbest lgbm's error=0.0400\n",
            "[flaml.automl: 06-16 17:20:10] {1013} INFO - iteration 33, current learner xgboost\n",
            "[flaml.automl: 06-16 17:20:10] {1165} INFO -  at 7.1s,\tbest xgboost's error=0.0467,\tbest lgbm's error=0.0400\n",
            "[flaml.automl: 06-16 17:20:10] {1013} INFO - iteration 34, current learner lgbm\n",
            "[flaml.automl: 06-16 17:20:10] {1165} INFO -  at 7.1s,\tbest lgbm's error=0.0400,\tbest lgbm's error=0.0400\n",
            "[flaml.automl: 06-16 17:20:10] {1013} INFO - iteration 35, current learner rf\n",
            "[flaml.automl: 06-16 17:20:11] {1165} INFO -  at 8.4s,\tbest rf's error=0.0533,\tbest lgbm's error=0.0400\n",
            "[flaml.automl: 06-16 17:20:11] {1013} INFO - iteration 36, current learner extra_tree\n",
            "[flaml.automl: 06-16 17:20:12] {1165} INFO -  at 9.5s,\tbest extra_tree's error=0.0400,\tbest lgbm's error=0.0400\n",
            "[flaml.automl: 06-16 17:20:12] {1013} INFO - iteration 37, current learner xgboost\n",
            "[flaml.automl: 06-16 17:20:12] {1165} INFO -  at 9.6s,\tbest xgboost's error=0.0467,\tbest lgbm's error=0.0400\n",
            "[flaml.automl: 06-16 17:20:12] {1013} INFO - iteration 38, current learner xgboost\n",
            "[flaml.automl: 06-16 17:20:12] {1165} INFO -  at 9.6s,\tbest xgboost's error=0.0467,\tbest lgbm's error=0.0400\n",
            "[flaml.automl: 06-16 17:20:12] {1013} INFO - iteration 39, current learner lgbm\n",
            "[flaml.automl: 06-16 17:20:13] {1165} INFO -  at 9.7s,\tbest lgbm's error=0.0400,\tbest lgbm's error=0.0400\n",
            "[flaml.automl: 06-16 17:20:13] {1013} INFO - iteration 40, current learner xgboost\n",
            "[flaml.automl: 06-16 17:20:13] {1165} INFO -  at 9.7s,\tbest xgboost's error=0.0467,\tbest lgbm's error=0.0400\n",
            "[flaml.automl: 06-16 17:20:13] {1013} INFO - iteration 41, current learner xgboost\n",
            "[flaml.automl: 06-16 17:20:13] {1165} INFO -  at 9.8s,\tbest xgboost's error=0.0467,\tbest lgbm's error=0.0400\n",
            "[flaml.automl: 06-16 17:20:13] {1013} INFO - iteration 42, current learner lgbm\n",
            "[flaml.automl: 06-16 17:20:13] {1165} INFO -  at 9.8s,\tbest lgbm's error=0.0400,\tbest lgbm's error=0.0400\n",
            "[flaml.automl: 06-16 17:20:13] {1013} INFO - iteration 43, current learner xgboost\n",
            "[flaml.automl: 06-16 17:20:13] {1165} INFO -  at 9.8s,\tbest xgboost's error=0.0467,\tbest lgbm's error=0.0400\n",
            "[flaml.automl: 06-16 17:20:13] {1013} INFO - iteration 44, current learner xgboost\n",
            "[flaml.automl: 06-16 17:20:13] {1165} INFO -  at 9.9s,\tbest xgboost's error=0.0467,\tbest lgbm's error=0.0400\n",
            "[flaml.automl: 06-16 17:20:13] {1013} INFO - iteration 45, current learner xgboost\n",
            "[flaml.automl: 06-16 17:20:13] {1165} INFO -  at 9.9s,\tbest xgboost's error=0.0333,\tbest xgboost's error=0.0333\n",
            "[flaml.automl: 06-16 17:20:13] {1013} INFO - iteration 46, current learner xgboost\n",
            "[flaml.automl: 06-16 17:20:13] {1165} INFO -  at 10.0s,\tbest xgboost's error=0.0333,\tbest xgboost's error=0.0333\n",
            "[flaml.automl: 06-16 17:20:13] {1013} INFO - iteration 47, current learner catboost\n",
            "[flaml.automl: 06-16 17:20:13] {1165} INFO -  at 10.1s,\tbest catboost's error=0.0333,\tbest xgboost's error=0.0333\n",
            "[flaml.automl: 06-16 17:20:13] {1205} INFO - selected model: XGBClassifier(colsample_bylevel=0.6902766231016318,\n",
            "              colsample_bytree=0.7657293008018354, grow_policy='lossguide',\n",
            "              learning_rate=0.42830712534058824, max_depth=0, max_leaves=5,\n",
            "              min_child_weight=0.2924296818378054, n_estimators=6, n_jobs=-1,\n",
            "              objective='multi:softprob', reg_alpha=0.00285817466554831,\n",
            "              reg_lambda=2.32876649803287, subsample=1.0, tree_method='hist',\n",
            "              use_label_encoder=False, verbosity=0)\n",
            "[flaml.automl: 06-16 17:20:13] {963} INFO - fit succeeded\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[0.9206522  0.04071239 0.03863542]\n",
            " [0.91942585 0.04199015 0.03858395]\n",
            " [0.91942585 0.04199015 0.03858395]\n",
            " [0.91942585 0.04199015 0.03858395]\n",
            " [0.9206522  0.04071239 0.03863542]\n",
            " [0.9206522  0.04071239 0.03863542]\n",
            " [0.9206522  0.04071239 0.03863542]\n",
            " [0.9206522  0.04071239 0.03863542]\n",
            " [0.91942585 0.04199015 0.03858395]\n",
            " [0.91942585 0.04199015 0.03858395]\n",
            " [0.9206522  0.04071239 0.03863542]\n",
            " [0.9206522  0.04071239 0.03863542]\n",
            " [0.91942585 0.04199015 0.03858395]\n",
            " [0.91942585 0.04199015 0.03858395]\n",
            " [0.91124994 0.04553654 0.04321346]\n",
            " [0.91124994 0.04553654 0.04321346]\n",
            " [0.9206522  0.04071239 0.03863542]\n",
            " [0.9206522  0.04071239 0.03863542]\n",
            " [0.91124994 0.04553654 0.04321346]\n",
            " [0.9206522  0.04071239 0.03863542]\n",
            " [0.9206522  0.04071239 0.03863542]\n",
            " [0.9206522  0.04071239 0.03863542]\n",
            " [0.9206522  0.04071239 0.03863542]\n",
            " [0.9206522  0.04071239 0.03863542]\n",
            " [0.9206522  0.04071239 0.03863542]\n",
            " [0.90682566 0.05511917 0.03805518]\n",
            " [0.9206522  0.04071239 0.03863542]\n",
            " [0.9206522  0.04071239 0.03863542]\n",
            " [0.9206522  0.04071239 0.03863542]\n",
            " [0.91942585 0.04199015 0.03858395]\n",
            " [0.91942585 0.04199015 0.03858395]\n",
            " [0.9206522  0.04071239 0.03863542]\n",
            " [0.9206522  0.04071239 0.03863542]\n",
            " [0.91124994 0.04553654 0.04321346]\n",
            " [0.91942585 0.04199015 0.03858395]\n",
            " [0.90682566 0.05511917 0.03805518]\n",
            " [0.91124994 0.04553654 0.04321346]\n",
            " [0.9206522  0.04071239 0.03863542]\n",
            " [0.91942585 0.04199015 0.03858395]\n",
            " [0.9206522  0.04071239 0.03863542]\n",
            " [0.9206522  0.04071239 0.03863542]\n",
            " [0.91942585 0.04199015 0.03858395]\n",
            " [0.91942585 0.04199015 0.03858395]\n",
            " [0.9206522  0.04071239 0.03863542]\n",
            " [0.9206522  0.04071239 0.03863542]\n",
            " [0.91942585 0.04199015 0.03858395]\n",
            " [0.9206522  0.04071239 0.03863542]\n",
            " [0.91942585 0.04199015 0.03858395]\n",
            " [0.9206522  0.04071239 0.03863542]\n",
            " [0.9206522  0.04071239 0.03863542]\n",
            " [0.03944915 0.916598   0.04395285]\n",
            " [0.03944915 0.916598   0.04395285]\n",
            " [0.04803457 0.82198745 0.12997797]\n",
            " [0.03887776 0.9178061  0.04331622]\n",
            " [0.03944915 0.916598   0.04395285]\n",
            " [0.03887776 0.9178061  0.04331622]\n",
            " [0.05258088 0.8888354  0.05858377]\n",
            " [0.09452778 0.81214833 0.09332389]\n",
            " [0.03944915 0.916598   0.04395285]\n",
            " [0.04365684 0.9132423  0.04310083]\n",
            " [0.04262994 0.89176106 0.06560896]\n",
            " [0.03887776 0.9178061  0.04331622]\n",
            " [0.03795881 0.89611214 0.06592909]\n",
            " [0.03944915 0.916598   0.04395285]\n",
            " [0.03887776 0.9178061  0.04331622]\n",
            " [0.03944915 0.916598   0.04395285]\n",
            " [0.03887776 0.9178061  0.04331622]\n",
            " [0.03887776 0.9178061  0.04331622]\n",
            " [0.0386049  0.8969821  0.06441305]\n",
            " [0.03887776 0.9178061  0.04331622]\n",
            " [0.06306089 0.3060567  0.6308824 ]\n",
            " [0.03944915 0.916598   0.04395285]\n",
            " [0.04803457 0.82198745 0.12997797]\n",
            " [0.03944915 0.916598   0.04395285]\n",
            " [0.03944915 0.916598   0.04395285]\n",
            " [0.03944915 0.916598   0.04395285]\n",
            " [0.03730296 0.8667315  0.09596551]\n",
            " [0.05983123 0.44828454 0.49188426]\n",
            " [0.03887776 0.9178061  0.04331622]\n",
            " [0.03887776 0.9178061  0.04331622]\n",
            " [0.03887776 0.9178061  0.04331622]\n",
            " [0.03887776 0.9178061  0.04331622]\n",
            " [0.03887776 0.9178061  0.04331622]\n",
            " [0.08867    0.368469   0.54286104]\n",
            " [0.04365684 0.9132423  0.04310083]\n",
            " [0.05184203 0.8903974  0.05776057]\n",
            " [0.03944915 0.916598   0.04395285]\n",
            " [0.03944915 0.916598   0.04395285]\n",
            " [0.03887776 0.9178061  0.04331622]\n",
            " [0.03887776 0.9178061  0.04331622]\n",
            " [0.03887776 0.9178061  0.04331622]\n",
            " [0.03944915 0.916598   0.04395285]\n",
            " [0.03887776 0.9178061  0.04331622]\n",
            " [0.04365684 0.9132423  0.04310083]\n",
            " [0.03887776 0.9178061  0.04331622]\n",
            " [0.03887776 0.9178061  0.04331622]\n",
            " [0.03887776 0.9178061  0.04331622]\n",
            " [0.03944915 0.916598   0.04395285]\n",
            " [0.04365684 0.9132423  0.04310083]\n",
            " [0.03887776 0.9178061  0.04331622]\n",
            " [0.03878148 0.04787951 0.913339  ]\n",
            " [0.03816188 0.06309111 0.898747  ]\n",
            " [0.03884385 0.04634818 0.914808  ]\n",
            " [0.03884385 0.04634818 0.914808  ]\n",
            " [0.03884385 0.04634818 0.914808  ]\n",
            " [0.03884385 0.04634818 0.914808  ]\n",
            " [0.11655146 0.5338494  0.34959906]\n",
            " [0.03884385 0.04634818 0.914808  ]\n",
            " [0.03884385 0.04634818 0.914808  ]\n",
            " [0.03878148 0.04787951 0.913339  ]\n",
            " [0.03843386 0.05641373 0.90515244]\n",
            " [0.03884385 0.04634818 0.914808  ]\n",
            " [0.03884385 0.04634818 0.914808  ]\n",
            " [0.0458779  0.08680344 0.86731863]\n",
            " [0.03816188 0.06309111 0.898747  ]\n",
            " [0.03843386 0.05641373 0.90515244]\n",
            " [0.03884385 0.04634818 0.914808  ]\n",
            " [0.03878148 0.04787951 0.913339  ]\n",
            " [0.03884385 0.04634818 0.914808  ]\n",
            " [0.06126272 0.3891252  0.5496121 ]\n",
            " [0.03843386 0.05641373 0.90515244]\n",
            " [0.04478408 0.10857601 0.8466399 ]\n",
            " [0.03884385 0.04634818 0.914808  ]\n",
            " [0.0448657  0.1069513  0.84818304]\n",
            " [0.03878148 0.04787951 0.913339  ]\n",
            " [0.03843386 0.05641373 0.90515244]\n",
            " [0.07021964 0.22727947 0.7025009 ]\n",
            " [0.0448657  0.1069513  0.84818304]\n",
            " [0.03884385 0.04634818 0.914808  ]\n",
            " [0.07675699 0.20280758 0.72043544]\n",
            " [0.03884385 0.04634818 0.914808  ]\n",
            " [0.03878148 0.04787951 0.913339  ]\n",
            " [0.03884385 0.04634818 0.914808  ]\n",
            " [0.08076092 0.29058227 0.6286568 ]\n",
            " [0.08076092 0.29058227 0.6286568 ]\n",
            " [0.03884385 0.04634818 0.914808  ]\n",
            " [0.03878148 0.04787951 0.913339  ]\n",
            " [0.03884385 0.04634818 0.914808  ]\n",
            " [0.06996467 0.23008528 0.69995004]\n",
            " [0.03884385 0.04634818 0.914808  ]\n",
            " [0.03884385 0.04634818 0.914808  ]\n",
            " [0.03884385 0.04634818 0.914808  ]\n",
            " [0.03816188 0.06309111 0.898747  ]\n",
            " [0.03843386 0.05641373 0.90515244]\n",
            " [0.03878148 0.04787951 0.913339  ]\n",
            " [0.03884385 0.04634818 0.914808  ]\n",
            " [0.04594473 0.08547327 0.868582  ]\n",
            " [0.03884385 0.04634818 0.914808  ]\n",
            " [0.03878148 0.04787951 0.913339  ]\n",
            " [0.03816188 0.06309111 0.898747  ]]\n",
            "<flaml.model.XGBoostSklearnEstimator object at 0x7efd327ed610>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tp7inKVs1elX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}